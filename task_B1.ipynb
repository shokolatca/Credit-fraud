{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElDqjXGkdlHy"
      },
      "source": [
        "### Задача B — антифрод через proxy-label / anomaly detection\n",
        "**Цель:** выявлять *подозрительные* кредиты/заявки, похожие на схему “early default”.\n",
        "\n",
        "**Вариант B1 (proxy-label):**\n",
        "- Формируем прокси-метку мошенничества:\n",
        "  - `fraud_proxy = 1`, если `loan_status=Charged Off` и `payment_ratio < τ` (например, τ=0.1),\n",
        "  - иначе `fraud_proxy = 0`.\n",
        "- Обучаем модель, которая по **заявочным** признакам (без leakage) предсказывает вероятность `fraud_proxy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score, roc_auc_score, \n",
        "    precision_recall_curve, f1_score, recall_score,\n",
        "    precision_score, confusion_matrix, classification_report,\n",
        "    PrecisionRecallDisplay, RocCurveDisplay\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Для разреженных матриц\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Всего записей: 38576\n",
            "\n",
            "Распределение loan_status:\n",
            "loan_status\n",
            "Fully Paid     32145\n",
            "Charged Off     5333\n",
            "Current         1098\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('financial_loan.csv')\n",
        "\n",
        "print(f\"Всего записей: {len(df)}\")\n",
        "print(f\"\\nРаспределение loan_status:\")\n",
        "print(df['loan_status'].value_counts())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "СТАТИСТИКА ПО МЕТКАМ:\n",
            "Charged Off: 5333 (14.2%)\n",
            "Fraud proxy (Charged Off + payment_ratio < 0.1): 206 (0.55%)\n",
            "Из Charged Off как fraud: 3.9%\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dfB1 = df[df[\"loan_status\"].isin([\"Fully Paid\", \"Charged Off\"])].copy()\n",
        "dfB1[\"payment_ratio\"] = dfB1[\"total_payment\"] / dfB1[\"loan_amount\"]\n",
        "dfB1[\"y\"] = ((dfB1[\"loan_status\"] == \"Charged Off\") & (dfB1[\"payment_ratio\"] < 0.1)).astype(int)\n",
        "fraud_count = dfB1[\"y\"].sum()\n",
        "total_charged_off = (dfB1['loan_status'] == 'Charged Off').sum()\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"СТАТИСТИКА ПО МЕТКАМ:\")\n",
        "print(f\"Charged Off: {total_charged_off} ({total_charged_off/len(dfB1)*100:.1f}%)\")\n",
        "print(f\"Fraud proxy (Charged Off + payment_ratio < 0.1): {fraud_count} ({fraud_count/len(dfB1)*100:.2f}%)\")\n",
        "print(f\"Из Charged Off как fraud: {fraud_count/total_charged_off*100:.1f}%\")\n",
        "print(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "leak_cols_drop = [\n",
        "    \"total_payment\", \"last_payment_date\", \"next_payment_date\",\n",
        "    \"last_credit_pull_date\", \"id\", \"member_id\", \"application_type\",\n",
        "    \"loan_status\", \"payment_ratio\"  \n",
        "]\n",
        "dfB1 = dfB1.drop(columns=leak_cols_drop, errors='ignore')\n",
        "\n",
        "\n",
        "dfB1[\"issue_date\"] = pd.to_datetime(dfB1[\"issue_date\"], dayfirst=True)\n",
        "\n",
        "\n",
        "dates_to_drop = [\n",
        "    '2021-01-01', '2021-01-05', '2021-02-25', '2021-07-17',\n",
        "    '2021-11-19', '2021-09-02', '2021-07-22', '2021-12-02', '2021-12-12', '2021-02-02'\n",
        "]\n",
        "dates_to_drop = pd.to_datetime(dates_to_drop)\n",
        "dfB1 = dfB1[~dfB1[\"issue_date\"].isin(dates_to_drop)]\n",
        "\n",
        "\n",
        "dfB1[\"issue_month\"] = dfB1[\"issue_date\"].dt.month.astype(\"int16\")\n",
        "dfB1 = dfB1.drop(columns=[\"issue_date\"], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfB1[\"term_months\"] = dfB1[\"term\"].astype(str).str.extract(r\"(\\d+)\").astype(\"int16\")\n",
        "dfB1 = dfB1.drop(columns=[\"term\"], errors='ignore')\n",
        "\n",
        "\n",
        "p99_income = dfB1[\"annual_income\"].quantile(0.99)\n",
        "dfB1[\"annual_income_cap\"] = dfB1[\"annual_income\"].clip(upper=p99_income)\n",
        "dfB1[\"log_income\"] = np.log10(dfB1[\"annual_income_cap\"].replace(0, 1))\n",
        "dfB1 = dfB1.drop(columns=[\"annual_income\", \"annual_income_cap\"], errors='ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "большие значения у среднего кридитного рейтинга"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "grade_map = {g: i+1 for i, g in enumerate(list(\"ABCDEFG\"))}\n",
        "dfB1[\"sub_grade_num\"] = dfB1[\"sub_grade\"].astype(str).apply(\n",
        "    lambda s: 5*(grade_map.get(s[0], np.nan)-1) + int(s[1]) if len(s) >= 2 and s[0] in grade_map else np.nan\n",
        ")\n",
        "\n",
        "CENTER = 13     \n",
        "MAX_SCORE = 20  \n",
        "\n",
        "dfB1[\"sub_grade_bell\"] = (\n",
        "    MAX_SCORE - (dfB1[\"sub_grade_num\"] - CENTER).abs()\n",
        ")\n",
        "\n",
        "\n",
        "dfB1[\"sub_grade_bell\"] = dfB1[\"sub_grade_bell\"].clip(lower=0)\n",
        "dfB1[\"sub_grade_num\"] = dfB1[\"sub_grade_bell\"].copy()\n",
        "dfB1 = dfB1.drop(columns=[\"grade\", \"sub_grade\", \"sub_grade_bell\"], errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def annuity_payment(L, annual_rate, n_months):\n",
        "    r = annual_rate / 12.0\n",
        "    if r == 0:\n",
        "        return L / n_months\n",
        "    return L * (r * (1 + r)**n_months) / ((1 + r)**n_months - 1)\n",
        "\n",
        "dfB1[\"installment_expected\"] = dfB1.apply(\n",
        "    lambda row: annuity_payment(row[\"loan_amount\"], row[\"int_rate\"], row[\"term_months\"]),\n",
        "    axis=1\n",
        ")\n",
        "dfB1[\"installment_rel_err\"] = (dfB1[\"installment\"] - dfB1[\"installment_expected\"]) / dfB1[\"installment_expected\"].replace(0, np.nan)\n",
        "dfB1 = dfB1.drop(columns=[\"installment_expected\", \"installment\"], errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfB1[\"loan_to_income\"] = dfB1[\"loan_amount\"] / df[\"annual_income\"].replace(0, np.nan)\n",
        "dfB1 = dfB1.drop(columns=[\"loan_amount\"], errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_title(x):\n",
        "    if pd.isna(x):\n",
        "        return \"unknown\"\n",
        "    x = str(x).lower()\n",
        "    x = re.sub(r\"[^a-z\\s]\", \" \", x)\n",
        "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
        "    return x if x else \"unknown\"\n",
        "\n",
        "dfB1[\"emp_title_clean\"] = dfB1[\"emp_title\"].apply(clean_title)\n",
        "dfB1[\"emp_title_is_unknown\"] = (dfB1[\"emp_title_clean\"] == \"unknown\").astype(int)\n",
        "dfB1[\"emp_title_len\"] = dfB1[\"emp_title_clean\"].str.len()\n",
        "dfB1 = dfB1.drop(columns=[\"emp_title\"], errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Размерность после feature engineering: (37467, 17)\n",
            "Колонки: ['address_state', 'home_ownership', 'purpose', 'verification_status', 'dti', 'total_acc', 'y', 'issue_month', 'term_months', 'log_income', 'sub_grade_num', 'installment_rel_err', 'loan_to_income', 'emp_title_clean', 'emp_title_is_unknown', 'emp_title_len', 'emp_length_years']\n"
          ]
        }
      ],
      "source": [
        "def parse_emp_length(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    s = str(x).strip().lower()\n",
        "    if s in [\"n/a\", \"na\", \"none\", \"null\", \"\", \"unknown\"]:\n",
        "        return np.nan\n",
        "    if \"<\" in s:\n",
        "        return 0.5\n",
        "    if \"10\" in s:\n",
        "        return 10\n",
        "    m = re.search(r\"(\\d+)\", s)\n",
        "    return float(m.group(1)) if m else np.nan\n",
        "\n",
        "dfB1[\"emp_length_years\"] = dfB1[\"emp_length\"].apply(parse_emp_length)\n",
        "dfB1 = dfB1.drop(columns=[\"emp_length\"], errors='ignore')\n",
        "\n",
        "# Удаляем int_rate (использовали для расчёта)\n",
        "dfB1 = dfB1.drop(columns=[\"int_rate\"], errors='ignore')\n",
        "\n",
        "print(f\"\\nРазмерность после feature engineering: {dfB1.shape}\")\n",
        "print(f\"Колонки: {list(dfB1.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Размеры выборок:\n",
            "Train: (22469, 17), Fraud rate: 0.58%\n",
            "Val: (7018, 17), Fraud rate: 0.58%\n",
            "Test: (7980, 17), Fraud rate: 0.43%\n"
          ]
        }
      ],
      "source": [
        "train_months = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "val_months = [9, 10]\n",
        "test_months = [11, 12]\n",
        "\n",
        "train_df = dfB1[dfB1[\"issue_month\"].isin(train_months)].copy()\n",
        "val_df = dfB1[dfB1[\"issue_month\"].isin(val_months)].copy()\n",
        "test_df = dfB1[dfB1[\"issue_month\"].isin(test_months)].copy()\n",
        "\n",
        "print(f\"\\nРазмеры выборок:\")\n",
        "print(f\"Train: {train_df.shape}, Fraud rate: {train_df['y'].mean()*100:.2f}%\")\n",
        "print(f\"Val: {val_df.shape}, Fraud rate: {val_df['y'].mean()*100:.2f}%\")\n",
        "print(f\"Test: {test_df.shape}, Fraud rate: {test_df['y'].mean()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "ver_map = {\"Not Verified\": 0, \"Source Verified\": 2, \"Verified\": 1}\n",
        "for d in [train_df, val_df, test_df]:\n",
        "    d[\"verif_ord\"] = d[\"verification_status\"].map(ver_map).fillna(-1).astype(int)\n",
        "    d.drop(\"verification_status\", axis=1, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def map_home(x):\n",
        "    x = str(x)\n",
        "    if x in [\"RENT\"]: return 2\n",
        "    if x in [\"MORTGAGE\"]: return 3\n",
        "    if x in [\"OWN\"]: return 1\n",
        "    return 0\n",
        "\n",
        "for d in [train_df, val_df, test_df]:\n",
        "    d[\"home_grp\"] = d[\"home_ownership\"].apply(map_home)\n",
        "    d.drop(\"home_ownership\", axis=1, inplace=True, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "topK = 8\n",
        "top_p = train_df[\"purpose\"].value_counts().head(topK).index\n",
        "\n",
        "for d in [train_df, val_df, test_df]:\n",
        "    d[\"purpose_grp\"] = d[\"purpose\"].where(d[\"purpose\"].isin(top_p), \"OTHER\")\n",
        "    \n",
        "grp2id = {g: i for i, g in enumerate(list(top_p) + [\"OTHER\"], start=1)}\n",
        "\n",
        "for d in [train_df, val_df, test_df]:\n",
        "    d[\"purpose_grp_id\"] = d[\"purpose_grp\"].map(grp2id).fillna(grp2id[\"OTHER\"])\n",
        "    d.drop([\"purpose\", \"purpose_grp\"], axis=1, inplace=True, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "NORTHEAST = set([\"CT\", \"ME\", \"MA\", \"NH\", \"RI\", \"VT\", \"NJ\", \"NY\", \"PA\"])\n",
        "MIDWEST = set([\"IL\", \"IN\", \"MI\", \"OH\", \"WI\", \"IA\", \"KS\", \"MN\", \"MO\", \"NE\", \"ND\", \"SD\"])\n",
        "SOUTH = set([\"DE\", \"FL\", \"GA\", \"MD\", \"NC\", \"SC\", \"VA\", \"DC\", \"WV\", \"AL\", \"KY\", \"MS\", \"TN\", \"AR\", \"LA\", \"OK\", \"TX\"])\n",
        "WEST = set([\"AZ\", \"CO\", \"ID\", \"MT\", \"NV\", \"NM\", \"UT\", \"WY\", \"AK\", \"CA\", \"HI\", \"OR\", \"WA\"])\n",
        "\n",
        "def state_region(s):\n",
        "    s = str(s)\n",
        "    if s in NORTHEAST: return \"NE\"\n",
        "    if s in MIDWEST: return \"MW\"\n",
        "    if s in SOUTH: return \"S\"\n",
        "    if s in WEST: return \"W\"\n",
        "    return \"UNK\"\n",
        "\n",
        "for d in [train_df, val_df, test_df]:\n",
        "    d[\"state_region\"] = d[\"address_state\"].apply(state_region)\n",
        "    \n",
        "reg_map = {\"NE\": 0, \"MW\": 1, \"S\": 2, \"W\": 3, \"UNK\": 4}\n",
        "for d in [train_df, val_df, test_df]:\n",
        "    d[\"state_region_ord\"] = d[\"state_region\"].map(reg_map).fillna(4).astype(int)\n",
        "    d.drop(columns=[\"state_region\", \"address_state\"], axis=1, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    min_df=20,\n",
        "    max_features=20000,\n",
        "    ngram_range=(1,2)\n",
        ")\n",
        "\n",
        "X_title_train = tfidf.fit_transform(train_df[\"emp_title_clean\"])\n",
        "X_title_val   = tfidf.transform(val_df[\"emp_title_clean\"])\n",
        "X_title_test  = tfidf.transform(test_df[\"emp_title_clean\"])\n",
        "\n",
        "train_df = train_df.drop(columns=[\"emp_title_clean\"], axis = 1)\n",
        "val_df   = val_df.drop(columns=[\"emp_title_clean\"], axis = 1)\n",
        "test_df  = test_df.drop(columns=[\"emp_title_clean\"], axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['address_state', 'home_ownership', 'purpose', 'verification_status',\n",
              "       'dti', 'total_acc', 'y', 'issue_month', 'term_months', 'log_income',\n",
              "       'sub_grade_num', 'installment_rel_err', 'loan_to_income',\n",
              "       'emp_title_clean', 'emp_title_is_unknown', 'emp_title_len',\n",
              "       'emp_length_years'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfB1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Финальные размерности:\n",
            "X_train: (22469, 490), y_train: (22469,), fraud: 131\n",
            "X_val: (7018, 490), y_val: (7018,), fraud: 41\n",
            "X_test: (7980, 490), y_test: (7980,), fraud: 34\n"
          ]
        }
      ],
      "source": [
        "y_train = train_df[\"y\"].values\n",
        "y_val = val_df[\"y\"].values\n",
        "y_test = test_df[\"y\"].values\n",
        "\n",
        "\n",
        "train_df = train_df.drop(columns=[\"y\"])\n",
        "val_df = val_df.drop(columns=[\"y\"])\n",
        "test_df = test_df.drop(columns=[\"y\"])\n",
        "\n",
        "\n",
        "for col in train_df.columns:\n",
        "    if train_df[col].dtype in ['float64', 'int64', 'int16']:\n",
        "        median_val = train_df[col].median()\n",
        "        train_df[col] = train_df[col].fillna(median_val)\n",
        "        val_df[col] = val_df[col].fillna(median_val)\n",
        "        test_df[col] = test_df[col].fillna(median_val)\n",
        "\n",
        "\n",
        "X_train_num = csr_matrix(train_df.values)\n",
        "X_val_num = csr_matrix(val_df.values)\n",
        "X_test_num = csr_matrix(test_df.values)\n",
        "\n",
        "X_train = hstack([X_train_num, X_title_train]).tocsr()\n",
        "X_val = hstack([X_val_num, X_title_val]).tocsr()\n",
        "X_test = hstack([X_test_num, X_title_test]).tocsr()\n",
        "\n",
        "print(f\"\\nФинальные размерности:\")\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}, fraud: {y_train.sum()}\")\n",
        "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}, fraud: {y_val.sum()}\")\n",
        "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}, fraud: {y_test.sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    average_precision_score, roc_auc_score, f1_score, \n",
        "    precision_recall_curve, confusion_matrix, precision_score, recall_score\n",
        ")\n",
        "\n",
        "def evaluate_model(model, X, y, dataset_name=\"Test\", threshold=0.5):\n",
        "    \"\"\"Оценка модели с выводом метрик и интерпретацией Precision/Recall\"\"\"\n",
        "    \n",
        "    # Предсказанные вероятности и классы\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X)[:, 1]\n",
        "    else:  # для моделей типа SVM\n",
        "        y_pred_proba = model.decision_function(X)\n",
        "    \n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "    \n",
        "    # Основные метрики\n",
        "    pr_auc = average_precision_score(y, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y, y_pred_proba)\n",
        "    f1 = f1_score(y, y_pred, zero_division=0)\n",
        "    \n",
        "    # Precision и Recall при текущем threshold\n",
        "    precision = precision_score(y, y_pred, zero_division=0)\n",
        "    recall = recall_score(y, y_pred, zero_division=0)\n",
        "    \n",
        "    # Recall при precision >= 0.5 (если возможно)\n",
        "    precision_vals, recall_vals, thresholds_pr = precision_recall_curve(y, y_pred_proba)\n",
        "    recall_at_precision_50 = 0\n",
        "    for p, r in zip(precision_vals, recall_vals):\n",
        "        if p >= 0.5:\n",
        "            recall_at_precision_50 = max(recall_at_precision_50, r)\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    \n",
        "    # Вывод\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"МЕТРИКИ: {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"PR-AUC (Average Precision): {pr_auc:.4f} ⭐ КЛЮЧЕВАЯ\")\n",
        "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}  → доля предсказанных fraud, которые реально fraud\")\n",
        "    print(f\"Recall: {recall:.4f}  → доля реальных fraud, которые модель поймала\")\n",
        "    print(f\"Recall@Precision≥0.5: {recall_at_precision_50:.4f}  → сколько fraud можно поймать, если хотим Precision ≥ 0.5\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"                 Pred 0   Pred 1\")\n",
        "    print(f\"Actual 0:      {cm[0,0]:6d}   {cm[0,1]:6d}  (Specificity: {cm[0,0]/(cm[0,0]+cm[0,1]):.3f})\")\n",
        "    print(f\"Actual 1:      {cm[1,0]:6d}   {cm[1,1]:6d}  (Recall: {cm[1,1]/(cm[1,0]+cm[1,1]) if (cm[1,0]+cm[1,1])>0 else 0:.3f})\")\n",
        "    \n",
        "    return {\n",
        "        'pr_auc': pr_auc,\n",
        "        'roc_auc': roc_auc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'recall_at_precision_50': recall_at_precision_50,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'y_pred': y_pred,\n",
        "        'cm': cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Словарь для хранения результатов\n",
        "all_results = {}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# БЕЗ БАЛАНСИРОВКИ (baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Logistic Regression (no balancing) ---\n",
            "\n",
            "============================================================\n",
            "МЕТРИКИ: LR Baseline\n",
            "============================================================\n",
            "PR-AUC (Average Precision): 0.0121 ⭐ КЛЮЧЕВАЯ\n",
            "ROC-AUC: 0.7009\n",
            "F1-score: 0.0000\n",
            "Precision (current threshold=0.5): 0.0000  → доля предсказанных fraud, которые реально fraud\n",
            "Recall (current threshold=0.5): 0.0000  → доля реальных fraud, которые модель поймала\n",
            "Recall@Precision≥0.5: 0.0000  → сколько fraud можно поймать, если хотим Precision ≥ 0.5\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred 0   Pred 1\n",
            "Actual 0:        7946        0  (Specificity: 1.000)\n",
            "Actual 1:          34        0  (Recall: 0.000)\n"
          ]
        }
      ],
      "source": [
        "# 10.1.1 Логистическая регрессия\n",
        "print(\"\\n--- Logistic Regression (no balancing) ---\")\n",
        "lr_baseline = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
        "lr_baseline.fit(X_train, y_train)\n",
        "res_lr_base = evaluate_model(lr_baseline, X_test, y_test, \"LR Baseline\")\n",
        "res_lr_base['y_true'] = y_test\n",
        "all_results['LR_Baseline'] = res_lr_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Decision Tree (no balancing) ---\n",
            "\n",
            "============================================================\n",
            "МЕТРИКИ: DT Baseline\n",
            "============================================================\n",
            "PR-AUC (Average Precision): 0.0054 ⭐ КЛЮЧЕВАЯ\n",
            "ROC-AUC: 0.4948\n",
            "F1-score: 0.0000\n",
            "Precision (current threshold=0.5): 0.0000  → доля предсказанных fraud, которые реально fraud\n",
            "Recall (current threshold=0.5): 0.0000  → доля реальных fraud, которые модель поймала\n",
            "Recall@Precision≥0.5: 0.0000  → сколько fraud можно поймать, если хотим Precision ≥ 0.5\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred 0   Pred 1\n",
            "Actual 0:        7946        0  (Specificity: 1.000)\n",
            "Actual 1:          34        0  (Recall: 0.000)\n"
          ]
        }
      ],
      "source": [
        "# 10.1.2 Decision Tree\n",
        "print(\"\\n--- Decision Tree (no balancing) ---\")\n",
        "dt_baseline = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_leaf=50)\n",
        "dt_baseline.fit(X_train, y_train)\n",
        "res_dt_base = evaluate_model(dt_baseline, X_test, y_test, \"DT Baseline\")\n",
        "res_dt_base['y_true'] = y_test\n",
        "all_results['DT_Baseline'] = res_dt_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- HistGradientBoosting (no balancing) ---\n",
            "\n",
            "============================================================\n",
            "МЕТРИКИ: HGB Baseline\n",
            "============================================================\n",
            "PR-AUC (Average Precision): 0.0062 ⭐ КЛЮЧЕВАЯ\n",
            "ROC-AUC: 0.6222\n",
            "F1-score: 0.0000\n",
            "Precision (current threshold=0.5): 0.0000  → доля предсказанных fraud, которые реально fraud\n",
            "Recall (current threshold=0.5): 0.0000  → доля реальных fraud, которые модель поймала\n",
            "Recall@Precision≥0.5: 0.0000  → сколько fraud можно поймать, если хотим Precision ≥ 0.5\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred 0   Pred 1\n",
            "Actual 0:        7938        8  (Specificity: 0.999)\n",
            "Actual 1:          34        0  (Recall: 0.000)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 10.1.3 HistGradientBoosting\n",
        "print(\"\\n--- HistGradientBoosting (no balancing) ---\")\n",
        "hgb_baseline = HistGradientBoostingClassifier(random_state=42, max_iter=100, early_stopping=True, \n",
        "                                               validation_fraction=0.1, n_iter_no_change=10)\n",
        "hgb_baseline.fit(X_train.toarray(), y_train)\n",
        "res_hgb_base = evaluate_model(hgb_baseline, X_test.toarray(), y_test, \"HGB Baseline\")\n",
        "res_hgb_base['y_true'] = y_test\n",
        "all_results['HGB_Baseline'] = res_hgb_base\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "балансировка + oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер после ROS: (23008, 490), распределение: [22338   670]\n"
          ]
        }
      ],
      "source": [
        "ros = RandomOverSampler(sampling_strategy=0.03, random_state=42)  # fraud = 3% от датасета\n",
        "X_train_ros, y_train_ros = ros.fit_resample(X_train.toarray(), y_train)\n",
        "\n",
        "print(f\"Размер после ROS: {X_train_ros.shape}, распределение: {np.bincount(y_train_ros)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Logistic Regression (балансировка + oversampling) ---\n",
            "\n",
            "============================================================\n",
            "МЕТРИКИ: LR Balanced + Oversampling\n",
            "============================================================\n",
            "PR-AUC (Average Precision): 0.2335 ⭐ КЛЮЧЕВАЯ\n",
            "ROC-AUC: 0.9093\n",
            "F1-score: 0.3070\n",
            "Precision (current threshold=0.5): 0.1962  → доля предсказанных fraud, которые реально fraud\n",
            "Recall (current threshold=0.5): 0.7060  → доля реальных fraud, которые модель поймала\n",
            "Recall@Precision≥0.5: 0.0000  → сколько fraud можно поймать, если хотим Precision ≥ 0.5\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred 0   Pred 1\n",
            "Actual 0:       20400     1938  (Specificity: 0.913)\n",
            "Actual 1:         197      473  (Recall: 0.706)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Logistic Regression (балансировка + oversampling) ---\")\n",
        "lr_balanced = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1, class_weight={0:1, 1:15})\n",
        "lr_balanced.fit(X_train_ros, y_train_ros)\n",
        "res_lr_bal = evaluate_model(lr_balanced, X_train_ros, y_train_ros, \"LR Balanced + Oversampling\")\n",
        "res_lr_bal['y_true'] = y_test\n",
        "all_results['LR_Balanced + Oversampling'] = res_lr_bal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Decision Tree (балансировка + oversampling) ---\n",
            "\n",
            "============================================================\n",
            "МЕТРИКИ: DT Balanced\n",
            "============================================================\n",
            "PR-AUC (Average Precision): 0.0049 ⭐ КЛЮЧЕВАЯ\n",
            "ROC-AUC: 0.4524\n",
            "F1-score: 0.0159\n",
            "Precision (current threshold=0.5): 0.0085  → доля предсказанных fraud, которые реально fraud\n",
            "Recall (current threshold=0.5): 0.1176  → доля реальных fraud, которые модель поймала\n",
            "Recall@Precision≥0.5: 0.0000  → сколько fraud можно поймать, если хотим Precision ≥ 0.5\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred 0   Pred 1\n",
            "Actual 0:        7482      464  (Specificity: 0.942)\n",
            "Actual 1:          30        4  (Recall: 0.118)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Decision Tree (балансировка + oversampling) ---\")\n",
        "weights = {0:1, 1:15}\n",
        "dt_balanced = DecisionTreeClassifier(\n",
        "    random_state=42,\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=50,\n",
        "    class_weight=weights\n",
        ")\n",
        "dt_balanced.fit(X_train_ros, y_train_ros)\n",
        "res_dt_bal = evaluate_model(dt_balanced, X_test, y_test, \"DT Balanced\")\n",
        "res_dt_bal['y_true'] = y_test\n",
        "all_results['DT_Balanced + Oversampling'] = res_dt_bal"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HistGradientBoosting class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- HistGradientBoosting (балансировка) ---\n",
            "\n",
            "============================================================\n",
            "МЕТРИКИ: HGB + ROS\n",
            "============================================================\n",
            "PR-AUC (Average Precision): 0.0078 ⭐ КЛЮЧЕВАЯ\n",
            "ROC-AUC: 0.6538\n",
            "F1-score: 0.0000\n",
            "Precision (current threshold=0.5): 0.0000  → доля предсказанных fraud, которые реально fraud\n",
            "Recall (current threshold=0.5): 0.0000  → доля реальных fraud, которые модель поймала\n",
            "Recall@Precision≥0.5: 0.0000  → сколько fraud можно поймать, если хотим Precision ≥ 0.5\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred 0   Pred 1\n",
            "Actual 0:        7874       72  (Specificity: 0.991)\n",
            "Actual 1:          34        0  (Recall: 0.000)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- HistGradientBoosting (балансировка) ---\")\n",
        "hgb_balanced = HistGradientBoostingClassifier(\n",
        "    max_iter=200,\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=20,\n",
        "    random_state=42,\n",
        "    class_weight={0:1, 1:50}  \n",
        ")\n",
        "hgb_balanced.fit(X_train.toarray(), y_train)\n",
        "res_hgb_balanced = evaluate_model(hgb_balanced, X_test.toarray(), y_test, \"HGB + ROS\")\n",
        "res_hgb_balanced['y_true'] = y_test\n",
        "all_results['HGB_balanced'] = res_hgb_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- HistGradientBoosting (Random Oversampling) ---\n",
            "\n",
            "============================================================\n",
            "МЕТРИКИ: HGB + ROS\n",
            "============================================================\n",
            "PR-AUC (Average Precision): 0.0076 ⭐ КЛЮЧЕВАЯ\n",
            "ROC-AUC: 0.5909\n",
            "F1-score: 0.0000\n",
            "Precision (current threshold=0.5): 0.0000  → доля предсказанных fraud, которые реально fraud\n",
            "Recall (current threshold=0.5): 0.0000  → доля реальных fraud, которые модель поймала\n",
            "Recall@Precision≥0.5: 0.0000  → сколько fraud можно поймать, если хотим Precision ≥ 0.5\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred 0   Pred 1\n",
            "Actual 0:        7945        1  (Specificity: 1.000)\n",
            "Actual 1:          34        0  (Recall: 0.000)\n"
          ]
        }
      ],
      "source": [
        "# 10.3.3 HistGradientBoosting\n",
        "print(\"\\n--- HistGradientBoosting (Random Oversampling) ---\")\n",
        "hgb_ros = HistGradientBoostingClassifier(random_state=42, max_iter=100)\n",
        "hgb_ros.fit(X_train_ros, y_train_ros)\n",
        "res_hgb_ros = evaluate_model(hgb_ros, X_test.toarray(), y_test, \"HGB + ROS\")\n",
        "res_hgb_ros['y_true'] = y_test\n",
        "all_results['HGB_ROS'] = res_hgb_ros"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "smote = SMOTE(sampling_strategy=0.03, random_state=42, k_neighbors=3)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train.toarray(), y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "МЕТРИКИ: LR Balanced + SMOTE\n",
            "============================================================\n",
            "PR-AUC (Average Precision): 0.2108 ⭐ КЛЮЧЕВАЯ\n",
            "ROC-AUC: 0.9135\n",
            "F1-score: 0.3040\n",
            "Precision (current threshold=0.5): 0.1933  → доля предсказанных fraud, которые реально fraud\n",
            "Recall (current threshold=0.5): 0.7104  → доля реальных fraud, которые модель поймала\n",
            "Recall@Precision≥0.5: 0.0000  → сколько fraud можно поймать, если хотим Precision ≥ 0.5\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred 0   Pred 1\n",
            "Actual 0:       20352     1986  (Specificity: 0.911)\n",
            "Actual 1:         194      476  (Recall: 0.710)\n"
          ]
        }
      ],
      "source": [
        "lr_balanced_smote = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1, class_weight={0:1, 1:15})\n",
        "lr_balanced_smote.fit(X_train_smote, y_train_smote)\n",
        "res_lr_bal_smote = evaluate_model(lr_balanced_smote, X_train_smote, y_train_smote, \"LR Balanced + SMOTE\")\n",
        "res_lr_bal_smote['y_true'] = y_test\n",
        "all_results['LR_Balanced + SMOTE'] = res_lr_bal_smote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                     Model   PR-AUC  ROC-AUC       F1   Recall  Precision:\n",
            "LR_Balanced + Oversampling 0.233492 0.909295 0.307043 0.705970    0.196184\n",
            "       LR_Balanced + SMOTE 0.210760 0.913526 0.303959 0.710448    0.193339\n",
            "               LR_Baseline 0.012066 0.700937 0.000000 0.000000    0.000000\n",
            "              HGB_balanced 0.007826 0.653751 0.000000 0.000000    0.000000\n",
            "                   HGB_ROS 0.007555 0.590941 0.000000 0.000000    0.000000\n",
            "              HGB_Baseline 0.006181 0.622172 0.000000 0.000000    0.000000\n",
            "               DT_Baseline 0.005365 0.494831 0.000000 0.000000    0.000000\n",
            "DT_Balanced + Oversampling 0.004885 0.452423 0.015936 0.117647    0.008547\n"
          ]
        }
      ],
      "source": [
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        'Model': name,\n",
        "        'PR-AUC': res['pr_auc'],\n",
        "        'ROC-AUC': res['roc_auc'],\n",
        "        'F1': res['f1'],\n",
        "        'Recall': res['recall'],\n",
        "        'Precision:': res['precision']\n",
        "        \n",
        "    }\n",
        "    for name, res in all_results.items()\n",
        "])\n",
        "\n",
        "results_df = results_df.sort_values('PR-AUC', ascending=False)\n",
        "print(\"\\n\" + results_df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (Anaconda)",
      "language": "python",
      "name": "anaconda"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
